{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실무에서는 대규모의 데이터를 기반으로 분류예측 모형을 만들어야 하는 경우가 많다. \n",
    "\n",
    "#### 대규모의 데이터라고 하면 표본의 갯수가 많거나 아니면 독립변수 즉, 특징 데이터의 종류가 많거나 혹은 이 두가지 모두인 경우가 있다. \n",
    "\n",
    "여기에서는 \n",
    "#### 특징데이터의 종류가 많은 경우에 가장 중요하다고 생각되는 특징데이터만 선택하여 특징데이터의 종류를 줄이기 위한 방법에 대해 알아본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "path = \"/Library/Fonts/NanumGothic.otf\"\n",
    "font_name = fm.FontProperties(fname=path, size=20).get_name()\n",
    "\n",
    "plt.rc('font', family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 s, sys: 2.33 s, total: 12.8 s\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.datasets import fetch_rcv1\n",
    "rcv_train = fetch_rcv1(subset=\"train\")\n",
    "rcv_test = fetch_rcv1(subset=\"test\")\n",
    "X_train = rcv_train.data  \n",
    "y_train = rcv_train.target  # 23149 x 103\n",
    "X_test = rcv_test.data\n",
    "y_test = rcv_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23149, 47236)\n"
     ]
    }
   ],
   "source": [
    "# One-Hot-Encoding된 라벨을 정수형으로 복원\n",
    "classes = np.arange(rcv_train.target.shape[1]) # 103개 클래스\n",
    "y_train = y_train.dot(classes)\n",
    "y_test = y_test.dot(classes)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분산에 의한 선택\n",
    "\n",
    "#### 원래 예측모형에서 중요한 특징데이터란 종속데이터와의 상관관계가 크고 예측에 도움이 되는 데이터를 말한다. \n",
    "\n",
    "#### 하지만 상관관계 계산에 앞서 특징데이터의 값 자체가 표본에 따라 그다지 변하지 않는다면 종속데이터 예측에도 도움이 되지 않을 가능성이 높다. \n",
    "\n",
    "#### 따라서 표본 변화에 따른 데이터 값의 변화 즉, 분산이 기준치보다 낮은 특징 데이터는 사용하지 않는 방법이 분산에 의한 선택 방법이다. \n",
    "\n",
    "예를 들어 종속데이터와 특징데이터가 모두 0 또는 1 두가지 값만 가지는데 종속데이터는 0과 1이 균형을 이루는데 반해 특징데이터가 대부분(예를 들어 90%)의 값이 0이라면 이 특징데이터는 분류에 도움이 되지 않을 가능성이 높다.\n",
    "\n",
    "하지만 분산에 의한 선택은 반드시 상관관계와 일치한다는 보장이 없기 때문에 신중하게 사용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23149, 14330)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector = VarianceThreshold(1e-5)\n",
    "X_train_sel = selector.fit_transform(X_train)\n",
    "X_test_sel = selector.transform(X_test)  # fit_transform을 하면 이전 fit 한 것을 초기화시킴\n",
    "X_train_sel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "독립변수가 47236 중 14330개가 선택되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.381\n",
      "test accuracy: 0.324\n",
      "CPU times: user 36.9 s, sys: 11.9 s, total: 48.8 s\n",
      "Wall time: 54.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"train accuracy: {:5.3f}\".format(accuracy_score(y_train, model.predict(X_train))))\n",
    "print(\"test accuracy: {:5.3f}\".format(accuracy_score(y_test, model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도가 높지 않다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.529\n",
      "test accuracy: 0.441\n",
      "CPU times: user 29.6 s, sys: 6.19 s, total: 35.8 s\n",
      "Wall time: 37.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train_sel, y_train)\n",
    "print(\"train accuracy: {:5.3f}\".format(accuracy_score(y_train, model.predict(X_train_sel))))\n",
    "print(\"test accuracy: {:5.3f}\".format(accuracy_score(y_test, model.predict(X_test_sel))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도가 조금 개선되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단일 변수 선택\n",
    "\n",
    "#### 단일 변수 선택법은 각각의 독립변수를 하나만 사용한 예측모형의 성능을 이용하여 가장 분류성능 혹은 상관관계가 높은 변수만 선택하는 방법이다. \n",
    "\n",
    "사이킷런 패키지의 feature_selection 서브패키지는 다음 성능지표를 제공한다.\n",
    "\n",
    "+ chi2: 카이제곱 검정 통계값\n",
    "+ f_classif: 분산분석(ANOVA) F검정 통계값\n",
    "+ mutual_info_classif: 상호정보량(mutual information)\n",
    "\n",
    "하지만 단일 변수의 성능이 높은 특징만 모았을 때 전체 성능이 반드시 향상된다는 보장은 없다.\n",
    "\n",
    "feature_selection 서브패키지는 성능이 좋은 변수만 사용하는 전처리기인 SelectKBest 클래스도 제공한다. 사용법은 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2, SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.505\n",
      "test accuracy: 0.438\n",
      "CPU times: user 26.9 s, sys: 6.87 s, total: 33.8 s\n",
      "Wall time: 34.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "selector1 = SelectKBest(chi2, k=14330)  # k : 독립변수 숫자\n",
    "X_train1 = selector1.fit_transform(X_train, y_train)  # 특이하게 y_train도 같이 fit\n",
    "X_test1 = selector1.transform(X_test)\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train1, y_train)\n",
    "print(\"train accuracy: {:5.3f}\".format(accuracy_score(y_train, model.predict(X_train1))))\n",
    "print(\"test accuracy: {:5.3f}\".format(accuracy_score(y_test, model.predict(X_test1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23149, 14330)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다른 모형을 이용한 특성 중요도 계산\n",
    "\n",
    "#### 특성 중요도(feature importance)를 계산할 수 있는 랜덤포레스트 등 다른 모형을 사용하여 일단 특성을 선택하고 최종 분류는 다른 모형을 사용할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.3 s, sys: 2.38 s, total: 38.7 s\n",
      "Wall time: 40.7 s\n"
     ]
    }
   ],
   "source": [
    "# 랜덤포레스 모형을 사용하여 특성 선택\n",
    "%%time\n",
    "\n",
    "n_sample = 10000\n",
    "idx = np.random.choice(range(len(y_train)), n_sample)  # 10000개만 랜덤으로 선택\n",
    "model_sel = ExtraTreesClassifier(n_estimators=50).fit(X_train[idx, :], y_train[idx])\n",
    "selector = SelectFromModel(model_sel, prefit=True, max_features=14330)\n",
    "X_train_sel = selector.transform(X_train)\n",
    "X_test_sel = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.603\n",
      "test accuracy: 0.492\n",
      "CPU times: user 26.7 s, sys: 5.95 s, total: 32.7 s\n",
      "Wall time: 33.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 베르누이 나이브베이즈 모형을 통해 분류 분석\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train_sel, y_train)\n",
    "print(\"train accuracy: {:5.3f}\".format(accuracy_score(y_train, model.predict(X_train_sel))))\n",
    "print(\"test accuracy: {:5.3f}\".format(accuracy_score(y_test, model.predict(X_test_sel))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 방법들 중 가장 정확도가 높으나, 테스트 데이터에 대해서는 크게 개선되지 않음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
